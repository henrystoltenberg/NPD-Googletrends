{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import single game features made in\n",
    "import pickle\n",
    "with open('X.pickle', 'rb') as handle:\n",
    "    X = pickle.load(handle)\n",
    "with open('Y.pickle', 'rb') as handle:\n",
    "    Y = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the ML Model\n",
    "\n",
    "The approach to generating rankings for each month will be to create pairwise comparisons between games each month. Approach based on __[RankSVM](www.cs.cornell.edu/~tj/publications/joachims_02c.pdf)__. The new features are the differences of the features of the two game then it becomes a classification problem: will the first game rank better or the second. I won't allow for ties since there are no ties in the actual rankings and I won't include comparisons between 2 unranked games in the training data since the games could have actually sold dramatically differently but I don't have the data for that.\n",
    "\n",
    "The following creates X_new and Y_new which now just an ordered lists containing the feature vector (for X_new) and the comparison result (for Y_new) for a pair of games in some month; index_values is another ordered list which keeps track of the month, 1st game and 2nd game indices for X_new and Y_new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing pairwise comparison, ignores ties for unranked games\n",
    "(l,m,n) = X.shape\n",
    "X_new = []\n",
    "Y_new = []\n",
    "index_values = [] #saves the month and game indicies\n",
    "for i in range(0,48): #month range\n",
    "    for j in range(0,m): #1st game\n",
    "        r1 = Y[i,j]\n",
    "        if X[i,j,0]>-1 and list(X[i,j,1:5]) != [0,0,0,0]: #skip if game has not come out yet, or game with 0 relative search history\n",
    "            for k in range(j+1,m): #looping over 2nd game\n",
    "                if X[i,k,0]>-1 and list(X[i,k,1:5]) != [0,0,0,0]:\n",
    "                    r2 = Y[i,k]\n",
    "                    if r1>r2:\n",
    "                        y_pair = +1\n",
    "                        x_pair = list(X[i,j] - X[i,k])\n",
    "                        Y_new.append(y_pair)\n",
    "                        X_new.append(x_pair)\n",
    "                        index_values.append([i,j,k])\n",
    "                    if r1<r2:\n",
    "                        y_pair = -1\n",
    "                        x_pair = list(X[i,j] - X[i,k])\n",
    "                        Y_new.append(y_pair)\n",
    "                        X_new.append(x_pair)\n",
    "                        index_values.append([i,j,k])\n",
    "\n",
    "#y = -1 if game 1 ranked better, y = +1 if game 2 ranked better\n",
    "#X_new and Y_new are now just an ordered list containing the feature vector (for X_new) and the comparison result (for Y_new) for a pair of \n",
    "#games in some month. \n",
    "\n",
    "                \n",
    "                    \n",
    "            \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of letting sklearn using built in function to split between train and test sets, I will manually split by month so I can later predict top 10 games of a month that isn't in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "PRNG = np.random.rand(l)\n",
    "X_train = []\n",
    "Y_train = []\n",
    "index_values_train = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "index_values_test = []\n",
    "j = 0\n",
    "k = 0\n",
    "month_index = 0\n",
    "for i in range(0,l):\n",
    "    r = PRNG[i]\n",
    "    if r<=0.66:\n",
    "        while month_index == i and k < len(index_values):\n",
    "            X_train.append(X_new[k])\n",
    "            Y_train.append(Y_new[k])\n",
    "            index_values_train.append(index_values[k])\n",
    "            k = k+1\n",
    "            try:\n",
    "                month_index = index_values[k][0] \n",
    "            except:\n",
    "                break\n",
    "    else:\n",
    "        while month_index == i and k < len(index_values):\n",
    "            X_test.append(X_new[k])\n",
    "            Y_test.append(Y_new[k])\n",
    "            index_values_test.append(index_values[k])\n",
    "            k = k+1 \n",
    "            try:\n",
    "                month_index = index_values[k][0] \n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf')\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuned_parameters = [{'kernel':['rbf'],'gamma':[1e-3,1e-4],'C':[1,10,100,1000]}]\n",
    "#clf = GridSearchCV(svm.SVC(),tuned_parameters,cv=3,scoring = 'f1')\n",
    "#clf.fit(X_train, Y_train)\n",
    "#predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test f1: 0.9669968469439774\n",
      "test precision: 0.9685386513645879\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"test f1:\", str(f1_score(Y_test, predictions, average='macro')))\n",
    "from sklearn.metrics import precision_score\n",
    "print(\"test precision:\", str(precision_score(Y_test, predictions, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick out top 10 then sort\n",
    "import pickle\n",
    "with open('clf.pickle','wb') as handle:\n",
    "    pickle.dump(clf,handle,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('predictions.pickle','wb') as handle:\n",
    "    pickle.dump(predictions,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gamelist.pickle', 'rb') as handle:\n",
    "    gamelist = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bradley-Terry Model\n",
    "From paired comparisons, I will use the Bradley-Terry model to generate rankings. The model estimates the probability, $P(i>j)$ of a paired comparison based on $\\pi_i$, a numerical value representing \"value\" (or \"skill\" or whatever is appropriate for the context of the problem) of each game. The ordering of $\\pi_i$'s then ranks all the games. Below I use a simple algorithm built upon the Bradley-Terry Model to estimate the $\\pi_i$'s based on the outcomes of a series of pairwise comparisons.\n",
    "\n",
    "The Bradley-Terry model and algorithm are explained __[here](https://projecteuclid.org/euclid.aos/1079120141)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BTmodel (indices, Y, n, loops): #seems to rank known ones most accurately when looped multiple times\n",
    "    p = 100*np.ones(n)/n\n",
    "    for k in range(0,loops):\n",
    "        for j in range(0,len(Y)):\n",
    "            game1_index = indices[j][1]\n",
    "            game2_index = indices[j][2]\n",
    "            if Y[j] == -1: #game 1 wins\n",
    "                p[game1_index] = (p[game2_index]+p[game1_index])\n",
    "            else: #game 2 wins\n",
    "                p[game2_index] = (p[game2_index]+p[game1_index])\n",
    "            p = 100*p/p.sum() #renormalize\n",
    "    return p\n",
    "\n",
    "#def BTmodel (indices, Y, n, loops): #seems to do better when looped once\n",
    "#    p = 100*np.ones(n)/n\n",
    "#    for k in range(0,loops):\n",
    "#        j = 0\n",
    "#        for i in range(0,n):\n",
    "#            W = 0\n",
    "#            s = 0\n",
    "#            game1_index = i\n",
    "#            while game1_index == i and j < len(Y):\n",
    "#                game2_index = indices[j][2]\n",
    "#                if Y[j] == -1:\n",
    "#                    W = W+1\n",
    "#                    s = s+ 1/(p[game2_index]+p[game1_index])\n",
    "#                else:\n",
    "#                    s = s+ 1/(p[game2_index]+p[game1_index])\n",
    "#                j = j+1\n",
    "#                try:\n",
    "#                    game1_index = indices[j][1]\n",
    "#                except:\n",
    "#                    pass\n",
    "#                if s != 0:\n",
    "#                    p[i] = W/s\n",
    "#                    p = 100*p/p.sum() #renormalize\n",
    "#    return p\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the rankings for test months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "month = 1\n",
    "while month == 1:\n",
    "    count = count +1\n",
    "    month = index_values_test[count][0]\n",
    "\n",
    "indices_test = index_values_test[0:11417]\n",
    "yfortest = predictions[0:11417]\n",
    "#yfortest = Y_test[0:16516] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = BTmodel(indices_test,yfortest,len(gamelist),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = [('game','S29'),('p',float)]\n",
    "q = []\n",
    "for i in range(0,len(p)):\n",
    "    q.append((gamelist[i],p[i]))\n",
    "qarray = np.array(q,dtype = dtype)\n",
    "qsort = np.sort(qarray, order = 'p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'Shadow Of The Tomb Raider', 22.00999884),\n",
       " (b'FIFA 19', 12.51490614),\n",
       " (b'Destiny 2: Forsaken', 12.21133606),\n",
       " (b'Madden NFL 19', 12.1263797),\n",
       " (b'Fe', 6.24261539),\n",
       " (b'NHL 19', 6.19338533),\n",
       " (b'Naruto To Boruto: Shinobi Str', 5.69904555),\n",
       " (b\"Tom Clancy's Rainbow Six: Sie\", 5.46240467),\n",
       " (b'Grand Theft Auto V', 2.80175578),\n",
       " (b'Journey', 1.44235394),\n",
       " (b'Ark: Survival Evolved', 1.1896242),\n",
       " (b'Minecraft', 0.94522078),\n",
       " (b'Super Mario Odyssey', 0.63761211),\n",
       " (b'Strange Brigade', 0.60475247),\n",
       " (b'God of War', 0.60440303),\n",
       " (b'Enter the Gungeon', 0.60140172),\n",
       " (b'Far Cry 5', 0.57141972),\n",
       " (b'Mega Man 11', 0.57101923),\n",
       " (b'Detroit: Become Human', 0.51814311),\n",
       " (b'Super Mario Party', 0.48776623),\n",
       " (b'Kingdom Come: Deliverance', 0.4774594)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediction of Top 20 Games for September 2018\n",
    "list(qsort[::-1][0:21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 11416\n",
    "month = 11\n",
    "while month == 11:\n",
    "    count = count +1\n",
    "    month = index_values_test[count][0]\n",
    "indices_test2 = index_values_test[11416:23444]\n",
    "yfortest2 = predictions[11416:23444]\n",
    "#yfortest2 = Y_test[16533:32493]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = BTmodel(indices_test2,yfortest2,len(gamelist),35) #1 loop works best?\n",
    "\n",
    "dtype = [('game','S29'),('p',float)]\n",
    "q2 = []\n",
    "for i in range(0,len(p)):\n",
    "    q2.append((gamelist[i],p2[i]))\n",
    "qarray2 = np.array(q2,dtype = dtype)\n",
    "qsort2 = np.sort(qarray2, order = 'p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b\"Tom Clancy's Rainbow Six: Sie\", 5.10358297),\n",
       " (b'Agents of Mayhem', 4.30978915),\n",
       " (b\"Tom Clancy's Rainbow Six Sieg\", 4.10333301),\n",
       " (b\"Hellblade: Senua's Sacrifice\", 4.08659091),\n",
       " (b'The Surge', 4.03157331),\n",
       " (b'Valkyria Revolution', 3.97153789),\n",
       " (b'Sonic Mania', 3.93884321),\n",
       " (b\"Everybody's Golf\", 3.89017056),\n",
       " (b'Yakuza Kiwami', 3.86680679),\n",
       " (b'Ultra Street Fighter II: The ', 3.86530358),\n",
       " (b'Battlefield 1', 3.85680572),\n",
       " (b'Prey', 3.73670024),\n",
       " (b'Star Trek: Bridge Crew', 3.73085911),\n",
       " (b'Pyre', 3.71940178),\n",
       " (b'Gravity Rush 2', 3.70234016),\n",
       " (b'Tales of Berseria', 3.69649931),\n",
       " (b'DiRT 4', 3.66196023),\n",
       " (b'Rocket League', 3.6378944),\n",
       " (b'NieR: Automata', 3.61833506),\n",
       " (b'F1 2017', 3.59503253),\n",
       " (b'Minecraft', 3.44645506)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediction of Top 20 Games for August 2017\n",
    "list(qsort2[::-1][0:21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_new.pickle','wb') as handle:\n",
    "    pickle.dump(X_new,handle,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('Y_new.pickle','wb') as handle:\n",
    "    pickle.dump(Y_new,handle,protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
